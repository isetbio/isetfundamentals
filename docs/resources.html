<!DOCTYPE html>
<!-- This site was created in Webflow. https://webflow.com --><!-- Last Published: Fri Apr 12 2024 05:11:25 GMT+0000 (Coordinated Universal Time) -->
<html data-wf-domain="dalt-net-6f8bed.webflow.io" data-wf-page="65e72a2419de36acbf4422e6"
    data-wf-site="65e72a2419de36acbf4422ac" lang="en">

<head>
    <meta charset="utf-8" />
    <title>Projects</title>
    <meta content="Projects" property="og:title" />
    <meta content="Projects" property="twitter:title" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <meta content="Webflow" name="generator" />
    <link
        href="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/css/dalt-net-6f8bed.webflow.e342a28d1.css"
        rel="stylesheet" type="text/css" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous" />
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script
        type="text/javascript">WebFont.load({ google: { families: ["Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic"] } });</script>
    <script
        type="text/javascript">!function (o, c) { var n = c.documentElement, t = " w-mod-"; n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch") }(window, document);</script>
    <link href="https://assets-global.website-files.com/img/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <link href="https://assets-global.website-files.com/img/webclip.png" rel="apple-touch-icon" />
</head>

<body>
    <div data-collapse="medium" data-animation="default" data-duration="400" data-easing="ease" data-easing2="ease"
        role="banner" class="navigation-bar w-nav">
        <div class="w-container"><a href="#" class="brand-link w-nav-brand">
                <h1 class="brand-text">Dalt-Net</h1>
            </a>
            <nav role="navigation" class="navigation-menu w-nav-menu"><a href="/index.html" aria-current="page"
                class="navigation-link w-nav-link w--current">Home</a><a href="/resources.html"
                class="navigation-link w-nav-link">Resources</a></nav>
            <div class="hamburger-button w-nav-button">
                <div class="w-icon-nav-menu"></div>
            </div>
        </div>
    </div>
    <div class="hero-section centered">
        <div class="w-layout-blockcontainer w-container">
            <div>
                <h1 data-ix="fade-in-bottom-page-loads" class="hero-heading">Technology</h1>
            </div>
        </div>
    </div>
    <div class="w-container">
        <div class="section-title-group">
            <h2 class="section-heading-2 centered">Our Technology</h2>
            <div class="section-subheading center"><strong>We use innovative, recent technology to apply breakthroughs
                    in the field of Daltonization for Color vision deficient observers</strong></div>
        </div>
    </div>
    <div class="section">
        <div class="div-block-21">
            <h2 class="heading">Where does the process start?</h2>
        </div>
        <div id="w-node-b1b0f321-b41f-c8f3-4deb-fbfd6c14ac38-bf4422e6"
            class="w-layout-layout quick-stack-2 wf-layout-layout">
            <div id="w-node-b1b0f321-b41f-c8f3-4deb-fbfd6c14ac39-bf4422e6" class="w-layout-cell cell-3">
                <p class="paragraph-3">Color Vision Deficiency (color blindness) affects about 8% of men and 0.44% of
                    women around the world. That&#x27;s about 300 million people, all facing barriers in a world that is
                    becoming increasingly digital, and thus visually-oriented. Unlike how people usually perceive it,
                    CVD is not a single condition—it consists of a long, continuous spectrum of visual impairment. </p>
            </div>
            <div id="w-node-b1b0f321-b41f-c8f3-4deb-fbfd6c14ac3a-bf4422e6" class="w-layout-cell cell-5">
                <div class="div-block-6"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f161184d101966d8c8deab_redg.jpg"
                        loading="lazy" sizes="400px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f161184d101966d8c8deab_redg-p-500.jpg 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f161184d101966d8c8deab_redg.jpg 650w"
                        alt="" class="image-9" /></div>
            </div>
            <div id="w-node-_9172887f-fd2d-d8bf-657a-3b1f5ff28d28-bf4422e6" class="w-layout-cell cell">
                <div class="div-block-7"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f162067bbd46ea687082cb_287px-Cones_SMJ2_E.svg.png"
                        loading="lazy" alt="" class="image-3" /></div>
            </div>
            <div id="w-node-_6c4bec7a-2ea3-4d2f-2ba6-b239988583a2-bf4422e6" class="w-layout-cell cell-8">
                <p class="paragraph-2">When underlying genetics cause one type of photoreceptor from the eye to become
                    inhibited or simply not be present, a range of EM wavelengths become lost. As color is essentially
                    3-dimensional (due to having 3 photoreceptors), this means that losing a photoreceptor will turn
                    one&#x27;s color perception into 2 dimensions, called dichromacy.</p>
            </div>
            <div id="w-node-_676341ea-9487-dad9-ce31-3dee864e54f8-bf4422e6" class="w-layout-cell cell-2">
                <p class="paragraph-2">Translating into other color spaces, like LMS and YUV, this manifests in
                    &quot;confusion lines&quot; which represent the lines of colors that dichromatic observers cannot
                    distinguish. In the YUV color space, the intersection of the confusion lines is the &quot;confusion
                    point&quot;. This also means that the only color information that pure dichromats will get is the
                    &quot;angle&quot; of the color with respect to the confusion point (when represented in YUV space).
                    This helps represent and stabilize color spaces for the following process to enhance the images for
                    the color blind—&quot;Daltonization&quot;.</p>
            </div>
            <div id="w-node-_8e8cee2e-392c-9b5e-42d3-b195df4a9748-bf4422e6" class="w-layout-cell cell-6">
                <div class="div-block-8"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f164104a1b66d2b50e5a8f_Screenshot%202024-03-13%20at%201.29.41%E2%80%AFAM.png"
                        loading="lazy" sizes="400px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f164104a1b66d2b50e5a8f_Screenshot%202024-03-13%20at%201.29.41%E2%80%AFAM-p-500.png 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f164104a1b66d2b50e5a8f_Screenshot%202024-03-13%20at%201.29.41%E2%80%AFAM-p-800.png 800w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f164104a1b66d2b50e5a8f_Screenshot%202024-03-13%20at%201.29.41%E2%80%AFAM-p-1080.png 1080w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f164104a1b66d2b50e5a8f_Screenshot%202024-03-13%20at%201.29.41%E2%80%AFAM.png 1532w"
                        alt="" /><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f1640dc7081b67aef165ec_Screenshot%202024-03-13%20at%201.29.52%E2%80%AFAM.png"
                        loading="lazy" sizes="400px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f1640dc7081b67aef165ec_Screenshot%202024-03-13%20at%201.29.52%E2%80%AFAM-p-500.png 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f1640dc7081b67aef165ec_Screenshot%202024-03-13%20at%201.29.52%E2%80%AFAM-p-800.png 800w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f1640dc7081b67aef165ec_Screenshot%202024-03-13%20at%201.29.52%E2%80%AFAM-p-1080.png 1080w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f1640dc7081b67aef165ec_Screenshot%202024-03-13%20at%201.29.52%E2%80%AFAM.png 1536w"
                        alt="" /></div>
            </div>
            <div id="w-node-acff4098-920c-edd8-89fc-425cb05d0629-bf4422e6" class="w-layout-cell cell-9">
                <div class="div-block-20"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a7bb65ed0df881905142_Screenshot%202024-04-11%20201640.png"
                        loading="lazy" sizes="450px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a7bb65ed0df881905142_Screenshot%202024-04-11%20201640-p-500.png 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a7bb65ed0df881905142_Screenshot%202024-04-11%20201640.png 649w"
                        alt="" class="image-11" /></div>
            </div>
            <div id="w-node-_5d2c57f7-a756-237d-553f-d94104438657-bf4422e6" class="w-layout-cell cell-10">
                <div><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a7bec0a4ac00fbe69323_Screenshot%202024-04-11%20201653.png"
                        loading="lazy" sizes="450px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a7bec0a4ac00fbe69323_Screenshot%202024-04-11%20201653-p-500.png 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a7bec0a4ac00fbe69323_Screenshot%202024-04-11%20201653.png 650w"
                        alt="" class="image-12" /></div>
            </div>
        </div>
        <div class="div-block-19">
            <p class="paragraph-4">Another way of understanding dichromacy: the 2nd picture is the protanopic (red
                color-blind) simulation of the original color gamut (range) in the 1st. Note that the gamuts above (and
                any 2d gamuts) are a cross section of the entire color range of humans (as color is 3d!). However, in
                the 2d protanopic gamut, there is only 1 dimension represented as a point&#x27;s distance from the
                confusion point becomes irrelevant. When the gamut is extended to 3d, there would only be 2 dimensions
                for protanopes.</p>
        </div>
        <div class="div-block-22">
            <h2>What is Daltonization—currently?</h2>
        </div>
        <div class="div-block-19">
            <p class="paragraph-4">Daltonization is the overarching method to <strong>recolor </strong>images to make
                them understandable for CVD individuals. There are 4 criteria that I have defined: a) Contrast
                maintained/improved in the Daltonized image, b) Context-based consistency of colors retained across
                different images, c) Context-based naturalness or reason of color of real objects demonstrated in
                Daltonized images, d) Speed of algorithm able to run in real-time for some device.</p>
        </div>
        <div id="w-node-c691e490-22b4-b181-d8fa-b10cee2cce5d-bf4422e6"
            class="w-layout-layout quick-stack wf-layout-layout">
            <div id="w-node-c691e490-22b4-b181-d8fa-b10cee2cce5e-bf4422e6" class="w-layout-cell cell-3">
                <p class="paragraph-3">A common method of Daltonization is to rotate the &quot;hue&quot; of color
                    clusters around the white point (which has no hue). The white point is inside the color gamut, which
                    means that most colors will rotate to another color that is also inside the gamut. For protanopia,
                    red usually rotates towards blue. This means that the original red, which is essentially black for
                    CVD observers, turns to magenta for normal observers but blue for CVD observers.</p>
            </div>
            <div id="w-node-c691e490-22b4-b181-d8fa-b10cee2cce64-bf4422e6" class="w-layout-cell cell-5">
                <div class="div-block-6"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a8c0757ee14733597c4d_Screenshot_2024-04-11_at_8.21.05_PM.webp"
                        loading="lazy" sizes="420px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a8c0757ee14733597c4d_Screenshot_2024-04-11_at_8.21.05_PM-p-500.webp 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a8c0757ee14733597c4d_Screenshot_2024-04-11_at_8.21.05_PM-p-800.webp 800w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a8c0757ee14733597c4d_Screenshot_2024-04-11_at_8.21.05_PM.webp 862w"
                        alt="" class="image-13" /></div>
            </div>
            <div id="w-node-c691e490-22b4-b181-d8fa-b10cee2cce67-bf4422e6" class="w-layout-cell cell">
                <div class="div-block-7"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a3450003de3e87c6fbdf_Screenshot%202024-04-11%20195747.png"
                        loading="lazy" sizes="420px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a3450003de3e87c6fbdf_Screenshot%202024-04-11%20195747-p-500.png 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a3450003de3e87c6fbdf_Screenshot%202024-04-11%20195747-p-800.png 800w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a3450003de3e87c6fbdf_Screenshot%202024-04-11%20195747-p-1080.png 1080w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618a3450003de3e87c6fbdf_Screenshot%202024-04-11%20195747.png 1461w"
                        alt="" class="image-14" /></div>
            </div>
            <div id="w-node-c691e490-22b4-b181-d8fa-b10cee2cce6a-bf4422e6" class="w-layout-cell cell-7">
                <p class="paragraph-2">However, this type of static hue-rotation brings many disadvantages. For one,
                    while hue rotation can bring distinguishability to reds in the original image, it will certainly
                    turn previously distinguishable colors or objects no longer such. Additionally, maybe a blue apple
                    wouldn&#x27;t look too nice in the eyes of CVD observers. They would likely prefer a more muted
                    color for objects that is consistent with what the it looks like in the real world around them.
                    Thus, <a
                        href="https://www.researchgate.net/publication/348080962_A_content-dependent_Daltonization_algorithm_for_colour_vision_deficiencies_based_on_lightness_and_chroma_information"
                        inline="display:inline-block" target="_blank">in a newer method</a>, each color cluster is
                    rotated differently, in order to displace them from being on the same confusion line. This is shown
                    in the adjacent image.</p>
            </div>
            <div id="w-node-c691e490-22b4-b181-d8fa-b10cee2cce6d-bf4422e6" class="w-layout-cell cell-2">
                <p class="paragraph-2">Building on this, an <a
                        href="https://www.researchgate.net/publication/343266951_Color_vision_deficiency_datasets_recoloring_evaluation_using_GANs"
                        target="_blank">even newer study</a> used Generative Adversarial Networks (yes, deep learning!)
                    in order to represent Daltonization transformations. The authors designed a mini-recoloring
                    algorithm named an &quot;Improved Octree Quantification Method&quot; which similar to the method in
                    the above paragraph, and combined it with <a
                        href="https://www.researchgate.net/publication/3343749_Information_Preserving_Color_Transformation_for_Protanopia_and_Deuteranopia"
                        target="_blank">another study&#x27;s</a> algorithm (a fixed hue-rotation) in order to create a
                    dataset, which then trained on several image-to-image GAN networks. However, the issue here is that
                    GANs are slowww... especially if you are applying it on a 30 FPS video.</p>
            </div>
            <div id="w-node-c691e490-22b4-b181-d8fa-b10cee2cce70-bf4422e6" class="w-layout-cell cell-6">
                <div class="div-block-8"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618af6eb5c1b5cf8a0723fe_Screenshot%202024-04-11%20204947.png"
                        loading="lazy" sizes="400px"
                        srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618af6eb5c1b5cf8a0723fe_Screenshot%202024-04-11%20204947-p-500.png 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618af6eb5c1b5cf8a0723fe_Screenshot%202024-04-11%20204947-p-800.png 800w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618af6eb5c1b5cf8a0723fe_Screenshot%202024-04-11%20204947-p-1080.png 1080w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618af6eb5c1b5cf8a0723fe_Screenshot%202024-04-11%20204947-p-1600.png 1600w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/6618af6eb5c1b5cf8a0723fe_Screenshot%202024-04-11%20204947.png 1640w"
                        alt="" /></div>
            </div>
        </div>
        <div>
            <div class="div-block-10">
                <div class="w-layout-hflex flex-block-6">
                    <h2>How does Dalt-Net approach this?</h2>
                </div>
            </div>
            <div class="div-block-11">
                <p class="paragraph-4">As opposed to traditional methodology for Daltonization, which involves strict,
                    determinate calculations that decide the rotation of color or color clusters within an image,
                    Dalt-Net involves a hybrid deep-learning architecture. This is also different from the GAN-based
                    deep learning method.</p>
            </div>
            <div class="div-block-15"><img
                    src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f165084434504fb4e4919f_Screenshot%202024-03-13%20at%201.34.11%E2%80%AFAM.png"
                    loading="lazy" width="869"
                    sizes="(max-width: 479px) 100vw, (max-width: 767px) 95vw, (max-width: 991px) 93vw, 869px" alt=""
                    srcset="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f165084434504fb4e4919f_Screenshot%202024-03-13%20at%201.34.11%E2%80%AFAM-p-500.png 500w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f165084434504fb4e4919f_Screenshot%202024-03-13%20at%201.34.11%E2%80%AFAM-p-800.png 800w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f165084434504fb4e4919f_Screenshot%202024-03-13%20at%201.34.11%E2%80%AFAM-p-1080.png 1080w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f165084434504fb4e4919f_Screenshot%202024-03-13%20at%201.34.11%E2%80%AFAM-p-1600.png 1600w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f165084434504fb4e4919f_Screenshot%202024-03-13%20at%201.34.11%E2%80%AFAM-p-2000.png 2000w, https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65f165084434504fb4e4919f_Screenshot%202024-03-13%20at%201.34.11%E2%80%AFAM.png 2414w"
                    class="image-8" /></div>
            <div class="div-block-16">
                <p class="paragraph-7">Here are several issues in current algorithms, and how Dalt-NET resolves them.
                </p>
            </div>
            <div class="w-layout-hflex flex-block-8">
                <div class="columns-2 w-row">
                    <div class="column w-col w-col-6">
                        <p class="paragraph-3">1. <strong>Traditional approaches often follow a single
                                scheme</strong><br />However, what scheme recolors an image “best” (based on
                            Daltonization standards) is heavily dependent on image color tones and lighting conditions.
                            This is based on how the human brain perceives vision (which is interpreted mainly through
                            the opponent color theory). So one method cannot work for all images.</p>
                    </div>
                    <div class="column-2 w-col w-col-6">
                        <p class="paragraph-3"><strong>2. Both traditional and GAN frameworks perform
                                slowly</strong><br />Traditional methods require searching for color clusters, which has
                            a time complexity scaling based on image size (or volume for videos). Additionally,
                            evaluation of GAN requires recursive calculations based on each pixel, and no explicit
                            transformation can be generalized to all pixels which would increase speed.</p>
                    </div>
                </div>
            </div>
            <div class="w-layout-hflex flex-block-8">
                <div class="columns-2 w-row">
                    <div class="column w-col w-col-6">
                        <p class="paragraph-3">1. <strong>Dalt-NET represents high dimensional and generalized data
                                through 3D Lookup Tables</strong><br />Dalt-NET is able to represent distinct and
                            separable methods through its custom dataset design, which has been partitioned into a
                            traditional (hue-rotation-based) data for baseline accuracy and novel (usually specialized
                            and context-based recoloring) data. Additionally, it generates multiple 3DLUTs (3D Lookup
                            Tables) to create adaptive channels based on the data, while maintaining a fixed 3DLUT as a
                            performance failsafe.</p>
                    </div>
                    <div class="column-2 w-col w-col-6">
                        <p class="paragraph-3"><strong>2. Dalt-NET evaluates model based on downsampled
                                image</strong><br />Dalt-NET&#x27;s weight predictor model is a simple, lightweight
                            convolutional network whose goal is to simply process the color tones and lighting
                            conditions of the original image. Specific object detection is not necessary. Thus, it can
                            operate on low-resolution samples. This allows the convolutional network to output at
                            near-constant time complexity, without hindering accuracy. In addition, the use of 3DLUTs
                            avoids heavy algorithm-specific calculations by the computer.</p>
                    </div>
                </div>
            </div>
        </div>
        <div class="div-block-11">
            <p class="paragraph-4">Finally, check out Dalt-NET&#x27;s application on this page! <a href="/about">Click
                    here</a></p>
        </div>
    </div>
    <div>
        <div class="container-2 w-container">
            <div class="w-row">
                <div class="spc w-col w-col-6">
                    <h5>about dalt-net</h5>
                    <p>Dalt-Net utilizes a deep-learning guided image transformation to modify visual content, like
                        images and GUI, in both digital software and physical interfaces. </p>
                </div>
                <div class="spc w-col w-col-6">
                    <h5>useful links</h5><a href="https://www.colourblindawareness.org/colour-blindness/"
                        class="footer-link">Intro to Color Blindness</a><a
                        href="https://ixora.io/projects/colorblindness/daltonization/" class="footer-link">What is
                        Daltonization?</a><a
                        href="https://www.academia.edu/83726227/Recoloring_Algorithms_for_Colorblind_People"
                        class="footer-link">Survey of Daltonization Algorithms</a><a
                        href="https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-computer-vision#object-classification"
                        class="footer-link">Primer into Computer Vision</a>
                </div>
            </div>
        </div>
    </div>
    <div class="section">
        <div class="container">
            <div class="footer-wrap"><a href="https://webflow.com/" target="_blank"
                    class="webflow-link w-inline-block"><img
                        src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/65e72a2419de36acbf442311_webflow-w-small%402x.png"
                        width="15" alt="" class="webflow-logo-tiny" />
                    <div class="paragraph-tiny">Powered by Webflow</div>
                </a></div>
        </div>
    </div>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=65e72a2419de36acbf4422ac"
        type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>
    <script src="https://assets-global.website-files.com/65e72a2419de36acbf4422ac/js/webflow.7c19a66de.js"
        type="text/javascript"></script>
</body>

</html>